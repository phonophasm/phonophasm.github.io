---
title: "Neo-Sense — Interactive Sense Swapping in VR"
layout: default
date: 2025-11-07
tags: [VR, Research, Interaction Design, Perception]
---

![Hero](/assets/projects/Neo-Sense/hero.jpg "Neo-Sense VR — overview")

## Overview
**Neo-Sense** is a VR research project exploring **sense swapping**—re-mapping how sight and hearing are delivered to the user—to challenge habitual perception and inspire new forms of interaction. Built in Unity for the Oculus/Meta ecosystem, Neo-Sense exchanges or rearranges visual and auditory channels so users must **search, navigate, and interact** using unusual sensory layouts. The result is a playful, disorienting, and ultimately enlightening perspective on **how we learn space through the senses**.

## Video Demo

<div class="video-container">
  <iframe
    width="560"
    height="315"
    src="https://www.youtube.com/embed/7nRPLwQgcok"
    title="Neo-Sense VR Demo"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen>
  </iframe>
</div>

---

## What “sense swapping” means (plain-English)
Instead of “eyes at the front, ears left/right all around,” Neo-Sense **reconfigures** both streams. Vision can be split to emphasize lateral views, while audio can be narrowed to a forward (or otherwise relocated) focus. Users then complete simple searching tasks, discovering new scanning strategies and adapting to the altered setup.

![System Diagram](/assets/projects/Neo-Sense/system%20diagram.png "System diagram: split visual channels + focused audio")

---

## Core interaction: Searching the space
Participants explore a colorful environment with **eight simple, distinct objects** (numbers, shapes, and clear cues). Tasks favor **low cognitive load** and high **sensory adaptation**: follow a sound to find its source, or use the split visual field to identify the correct target. The emphasis is on **embodied exploration** and **strategy-shifting** rather than puzzle difficulty.

![Interaction](/assets/projects/Neo-Sense/interaction.jpg "Searching task in the altered-sense environment")

---

## Three configurations (A/B/C)
To surface different adaptation behaviors, Neo-Sense offers three swap layouts:

- **A — Ears Front, Eyes Left & Right**  
  Audio is concentrated straight ahead; visual input emphasizes lateral perspectives. Great for noticing how people “sweep” side views while homing in on a forward audio cone.

- **B — Ears Back, Eyes Left & Right**  
  Sound arrives from behind while vision remains lateral. Users must reconcile backward cues with sideways scanning—provoking unusual head-turn patterns.

- **C — Ears Top, Eyes Front & Back**  
  Audio is “overhead” while vision covers both forward and rear directions. This setup challenges forward-bias habits and reveals how quickly users develop **new scanning rituals**.

![Configuration A](/assets/projects/Neo-Sense/head%20A.png "Configuration A — Ears Front, Eyes Left & Right")

![Configuration B](/assets/projects/Neo-Sense/head%20B.png "Configuration B — Ears Back, Eyes Left & Right")

![Configuration C](/assets/projects/Neo-Sense/head%20C.png "Configuration C — Ears Top, Eyes Front & Back")

---

## Why this matters
Neo-Sense suggests that small, deliberate changes to sensory routing can **increase engagement**, **shift navigation strategies**, and **highlight brain plasticity**. It’s a compact lens on **perception-as-skill**: users relearn how to search, align, and act when the usual sensory contract is rewritten.

---

## Design notes (condensed process)
- **Constraint-first**: Keep tasks simple so adaptation—not puzzle-solving—stays center stage.  
- **Embodied UX**: Favor large cues, distinct objects, and readable spatial feedback over complexity.  
- **Iteration**: Early prototypes tested different splits (visual lateralization vs. audio focus) to find combinations that are surprising but learnable.  
- **Observation**: Users quickly adopt wider head-sweeps for vision or rely more heavily on “audio cones,” revealing emergent strategies.

![Prototype](/assets/projects/Neo-Sense/prototype.png "Early prototype / lab testing")

---

## Tech stack
- **Engine**: Unity  
- **Device**: Oculus/Meta Quest (developed on Quest 2; principles apply to Quest 3)  
- **Implementation**: custom camera/audio routing for split vision and focused/relocated audio; lightweight environment & cues for consistent tests

---

## Applications & next steps
- **Education**: teach spatial hearing/vision concepts through lived experience.  
- **Therapy/Training**: safe environments for practicing adaptation and spatial awareness.  
- **Art & Entertainment**: new interaction grammars for exploratory games and installations.

**Planned updates**: more configurable swap modes, session logging/metrics, and a designer panel to author tasks & difficulty ramps.

---

## Acknowledgements
Research advised by **Professor Yen-Ting Cho** at **ICID, NCKU (Taiwan)**.

---

## Gallery
![Hero](/assets/projects/Neo-Sense/hero.jpg)
![System Diagram](/assets/projects/Neo-Sense/system%20diagram.png)
![Configuration A](/assets/projects/Neo-Sense/head%20A.png)
![Configuration B](/assets/projects/Neo-Sense/head%20B.png)
![Configuration C](/assets/projects/Neo-Sense/head%20C.png)
![Interaction](/assets/projects/Neo-Sense/interaction.jpg)
![Prototype](/assets/projects/Neo-Sense/prototype.png)

